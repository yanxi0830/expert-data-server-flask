{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HIPGUTRVnDPD"},"source":["# NeuralDataServer: FastAdapt Colab Notebook"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WioFVO_GFWSJ"},"source":["## Setup"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"zYb9trM9mHkP"},"outputs":[],"source":["######################################################################\n","# Setup python environment\n","######################################################################\n","!pip install torch torchvision\n","!pip install torchnet\n","%mkdir -p /content/fast-adapt\n","%mkdir -p /content/fast-adapt/dataset\n","%mkdir -p /content/fast-adapt/experts\n","\n","######################################################################\n","# Download and unzip experts\n","######################################################################\n","%cd /content/fast-adapt/experts\n","!wget https://www.dropbox.com/s/7odxi0rmutcnkwv/coco.zip\n","!mkdir coco\n","!unzip coco.zip -d coco\n","!wget https://www.dropbox.com/s/97kr7k3b2wyjjnn/openimages.zip\n","!mkdir openimages\n","!unzip openimages.zip -d openimages\n","\n","%cd /content/fast-adapt/"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"b4oewpf_oGEu"},"source":["## Utility Functions"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"6Edj0DGColzX"},"outputs":[],"source":["from __future__ import print_function\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torch.utils.data as data\n","import torchvision.datasets\n","from tqdm import tqdm\n","import torchvision.transforms as transforms\n","from torchvision.datasets.folder import default_loader, IMG_EXTENSIONS, has_file_allowed_extension\n","import os\n","import datetime\n","import logging\n","from collections import OrderedDict\n","import numpy as np\n","import random\n","import torchnet as tnt\n","from torch.utils.data.dataloader import default_collate\n","import pickle"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"PkLzCsf9oNDF"},"outputs":[],"source":["__all__ = ['ResNet', 'resnet18']\n","\n","\n","model_urls = {\n","    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n","    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n","    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n","    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n","    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n","    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n","    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n","    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n","    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n","}\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None):\n","        super(BasicBlock, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        if groups != 1 or base_width != 64:\n","            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n","        if dilation > 1:\n","            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n","        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = norm_layer(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = norm_layer(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None):\n","        super(Bottleneck, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        width = int(planes * (base_width / 64.)) * groups\n","        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv1x1(inplanes, width)\n","        self.bn1 = norm_layer(width)\n","        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n","        self.bn2 = norm_layer(width)\n","        self.conv3 = conv1x1(width, planes * self.expansion)\n","        self.bn3 = norm_layer(planes * self.expansion)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","\n","    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n","                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n","                 norm_layer=None):\n","        super(ResNet, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        self._norm_layer = norm_layer\n","\n","        self.inplanes = 64\n","        self.dilation = 1\n","        if replace_stride_with_dilation is None:\n","            # each element in the tuple indicates if we should replace\n","            # the 2x2 stride with a dilated convolution instead\n","            replace_stride_with_dilation = [False, False, False]\n","        if len(replace_stride_with_dilation) != 3:\n","            raise ValueError(\"replace_stride_with_dilation should be None \"\n","                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n","        self.groups = groups\n","        self.base_width = width_per_group\n","        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = norm_layer(self.inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n","                                       dilate=replace_stride_with_dilation[0])\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n","                                       dilate=replace_stride_with_dilation[1])\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n","                                       dilate=replace_stride_with_dilation[2])\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n","        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, Bottleneck):\n","                    nn.init.constant_(m.bn3.weight, 0)\n","                elif isinstance(m, BasicBlock):\n","                    nn.init.constant_(m.bn2.weight, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n","        norm_layer = self._norm_layer\n","        downsample = None\n","        previous_dilation = self.dilation\n","        if dilate:\n","            self.dilation *= stride\n","            stride = 1\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                conv1x1(self.inplanes, planes * block.expansion, stride),\n","                norm_layer(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n","                            self.base_width, previous_dilation, norm_layer))\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, groups=self.groups,\n","                                base_width=self.base_width, dilation=self.dilation,\n","                                norm_layer=norm_layer))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","\n","def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n","    model = ResNet(block, layers, **kwargs)\n","    if pretrained:\n","        state_dict = load_state_dict_from_url(model_urls[arch],\n","                                              progress=progress)\n","        model.load_state_dict(state_dict)\n","    return model\n","\n","\n","def resnet18(pretrained=False, progress=True, **kwargs):\n","    r\"\"\"ResNet-18 model from\n","    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n","                   **kwargs)\n"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"UMBnTNKBoWgo"},"outputs":[],"source":["class GenericFolder(torchvision.datasets.VisionDataset):\n","    \"\"\"\n","    Dataset for given image folder, no labels\n","    \"\"\"\n","    def __init__(self, root, loader=default_loader, extensions=IMG_EXTENSIONS, transform=None, target_transform=None,\n","                 is_valid_file=None, multi_level=False):\n","        super(GenericFolder, self).__init__(root)\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.multi_level = multi_level\n","        samples = self.make_dataset(self.root, extensions, is_valid_file)\n","        if len(samples) == 0:\n","            raise RuntimeError(\"Found 0 files in subfolders of: \" +\n","                               self.root + \"\\nSupported extensions are: \" + \",\".join(extensions))\n","\n","        self.loader = loader\n","        self.extensions = extensions\n","\n","        self.samples = samples\n","        self.targets = [s[1] for s in samples]\n","\n","    def __getitem__(self, index):\n","        \"\"\"\n","        Args:\n","            index (int): Index\n","\n","        Returns:\n","            tuple: (image, img_path) where img_path is the absolute file path to image\n","        \"\"\"\n","        path, target = self.samples[index]  # no labeled target => 0\n","        sample = self.loader(path)\n","        if self.transform is not None:\n","            sample = self.transform(sample)\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","        return sample, path\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def make_dataset(self, dir, extensions=None, is_valid_file=None):\n","        images = []\n","        dir = os.path.expanduser(dir)\n","        if not ((extensions is None) ^ (is_valid_file is None)):\n","            raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n","        if extensions is not None:\n","            def is_valid_file(x):\n","                return has_file_allowed_extension(x, extensions)\n","        d = dir\n","        if self.multi_level:\n","            for root, _, fnames in sorted(os.walk(d)):\n","                for fname in tqdm(sorted(fnames)):\n","                    path = os.path.join(root, fname)\n","                    if is_valid_file(path):\n","                        item = (path, 0)\n","                        images.append(item)\n","        else:\n","            for fname in tqdm(sorted(os.listdir(d))):\n","                path = os.path.join(dir, fname)\n","                if is_valid_file(path):\n","                    item = (path, 0)\n","                    images.append(item)\n","\n","        return images\n","\n","\n","class GenericDataset(data.Dataset):\n","    def __init__(self, config=None):\n","        self.config = config\n","\n","        self.mean_pix = [0.485, 0.456, 0.406]\n","        self.std_pix = [0.229, 0.224, 0.225]\n","\n","        transforms_list = [\n","            transforms.RandomCrop(224),\n","            transforms.RandomHorizontalFlip(),\n","            lambda x: np.asarray(x),\n","        ]\n","        self.transform = transforms.Compose(transforms_list)\n","        self.data = GenericFolder(root=config['image_directory'], transform=self.transform, multi_level=True)\n","\n","    def __getitem__(self, index):\n","        img, label = self.data[index]\n","        return img, -1\n","\n","    def __len__(self):\n","        return len(self.data)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"jckhjRAUo_2F"},"outputs":[],"source":["class Denormalize(object):\n","    def __init__(self, mean, std):\n","        self.mean = mean\n","        self.std = std\n","\n","    def __call__(self, tensor):\n","        for t, m, s in zip(tensor, self.mean, self.std):\n","            t.mul_(s).add_(m)\n","        return tensor\n","\n","\n","def rotate_img(img, rot):\n","    if rot == 0:  # 0 degrees rotation\n","        return img\n","    elif rot == 90:  # 90 degrees rotation\n","        return np.flipud(np.transpose(img, (1, 0, 2))).copy()\n","    elif rot == 180:  # 90 degrees rotation\n","        return np.fliplr(np.flipud(img)).copy()\n","    elif rot == 270:  # 270 degrees rotation\n","        return np.transpose(np.flipud(img), (1, 0, 2)).copy()\n","    else:\n","        raise ValueError('rotation should be 0, 90, 180, or 270 degrees')\n","\n","\n","class DataLoader(object):\n","    def __init__(self,\n","                 dataset,\n","                 batch_size=1,\n","                 epoch_size=None,\n","                 num_workers=0,\n","                 shuffle=True):\n","        self.dataset = dataset\n","        self.shuffle = shuffle\n","        self.epoch_size = epoch_size if epoch_size is not None else len(dataset)\n","        self.batch_size = batch_size\n","        self.num_workers = num_workers\n","\n","        mean_pix = self.dataset.mean_pix\n","        std_pix = self.dataset.std_pix\n","        self.transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=mean_pix, std=std_pix)\n","        ])\n","        self.inv_transform = transforms.Compose([\n","            Denormalize(mean_pix, std_pix),\n","            lambda x: x.numpy() * 255.0,\n","            lambda x: x.transpose(1, 2, 0).astype(np.uint8),\n","        ])\n","\n","    def get_iterator(self, epoch=0):\n","        rand_seed = epoch * self.epoch_size\n","        random.seed(rand_seed)\n","        def _load_function(idx):\n","            idx = idx % len(self.dataset)\n","            img0, _ = self.dataset[idx]\n","            rotated_imgs = [\n","                self.transform(img0),\n","                self.transform(rotate_img(img0, 90)),\n","                self.transform(rotate_img(img0, 180)),\n","                self.transform(rotate_img(img0, 270))\n","            ]\n","\n","            rotation_labels = torch.LongTensor([0, 1, 2, 3])\n","            return torch.stack(rotated_imgs, dim=0), rotation_labels\n","\n","        def _collate_fun(batch):\n","            batch = default_collate(batch)\n","            assert (len(batch) == 2)\n","            batch_size, rotations, channels, height, width = batch[0].size()\n","            batch[0] = batch[0].view([batch_size * rotations, channels, height, width])\n","            batch[1] = batch[1].view([batch_size * rotations])\n","            return batch\n","\n","        tnt_dataset = tnt.dataset.ListDataset(elem_list=range(self.epoch_size),\n","                                              load=_load_function)\n","        data_loader = tnt_dataset.parallel(batch_size=self.batch_size,\n","                                           collate_fn=_collate_fun, num_workers=self.num_workers,\n","                                           shuffle=self.shuffle)\n","        return data_loader\n","\n","    def __call__(self, epoch=0):\n","        return self.get_iterator(epoch)\n","\n","    def __len__(self):\n","        return int(self.epoch_size / self.batch_size)\n"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"EB_TATwsqCqz"},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0.0\n","        self.sum = 0.0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += float(val * n)\n","        self.count += n\n","        self.avg = round(self.sum / self.count, 4)\n","\n","\n","class DAverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.values = {}\n","\n","    def update(self, values):\n","        assert (isinstance(values, dict))\n","        for key, val in values.items():\n","            if isinstance(val, (float, int)):\n","                if not (key in self.values):\n","                    self.values[key] = AverageMeter()\n","                self.values[key].update(val)\n","            else:\n","              raise NotImplementedError()\n","\n","    def average(self):\n","        average = {}\n","        for key, val in self.values.items():\n","            if isinstance(val, type(self)):\n","                average[key] = val.average()\n","            else:\n","                average[key] = val.avg\n","\n","        return average\n","\n","    def __str__(self):\n","        ave_stats = self.average()\n","        return ave_stats.__str__()\n"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"5m083nLsqOuc"},"outputs":[],"source":"def accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n\n    return res\n\ndef remove_module_prefix(checkpoint):\n    state_dict = checkpoint['network']\n    pretrained_key_name = list(state_dict.items())[0][0]\n    if 'module' in pretrained_key_name:\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            name = k[7:]\n            new_state_dict[name] = v\n        checkpoint['network'] = new_state_dict\n        return checkpoint\n    else:\n        return checkpoint\n\ndef set_random_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VIRN4K0xLei_"},"source":["## Evaluation\n","\n","- Upload client images to `/fast-adapt/dataset` folder\n","- Set `config['experts_dir']` to be directory containing source dataset experts"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"pit3WT_KsLvV"},"outputs":[],"source":"config = {\n    \"image_directory\": \"./dataset\",\n    \"batch_size\": 16,\n    \"experts_dir\": \"./experts/openimages\"   \n}"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"Uc2PheDRqyRD"},"outputs":[],"source":["def evaluate(network, dataloader):\n","  eval_stats = DAverageMeter()\n","  for idx, batch in enumerate(tqdm(dataloader())):\n","    img, target = batch\n","    if torch.cuda.is_available():\n","      img, target = img.cuda(), target.cuda()\n","    pred = network(img)\n","    record = {}\n","    record['prec1'] = accuracy(pred.data, target, topk=(1,))[0].item()\n","\n","    eval_stats.update(record)\n","\n","  return eval_stats.average()\n","\n","set_random_seed(0)\n","\n","dataset = GenericDataset(config=config)\n","dataloader = DataLoader(dataset=dataset, batch_size=config['batch_size'], shuffle=False)\n","\n","z = {}\n","\n","for e in os.listdir(config['experts_dir']):\n","  e_path = os.path.join(config['experts_dir'], e)\n","  \n","  model = resnet18(num_classes=4)\n","  \n","  pretrained = torch.load(e_path)\n","  pretrained = remove_module_prefix(pretrained)\n","  model.load_state_dict(pretrained['network'])\n","\n","  if torch.cuda.is_available():\n","    model = model.cuda()\n","\n","  model.eval()\n","  eval_stats = evaluate(model, dataloader)\n","  z[e] = eval_stats['prec1']\n","  print(\"Expert {}: {}\".format(e, eval_stats))\n","\n","print(z)\n","\n","with open('z.pickle', 'wb') as f:\n","  pickle.dump(z, f, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9XEpU59fMKUr"},"source":["## Next Step\n","- Download `fast-adapt/z.pickle`\n","- Upload `z.pickle` to Neural Data Server"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}